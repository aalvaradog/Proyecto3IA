LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name        | Type             | Params
-------------------------------------------------
0 | autoencoder | UNetAutoencoder  | 31.0 M
1 | model       | VGG              | 134 M
2 | criterion   | CrossEntropyLoss | 0
-------------------------------------------------
119 M     Trainable params
45.8 M    Non-trainable params
165 M     Total params
661.708   Total estimated model params size (MB)
c:\Users\tian_\AppData\Local\anaconda3\envs\Pytorch_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
c:\Users\tian_\AppData\Local\anaconda3\envs\Pytorch_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.

Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
c:\Users\tian_\AppData\Local\anaconda3\envs\Pytorch_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.



Epoch 0: 100%|██████████| 19/19 [00:04<00:00,  4.19it/s, v_num=ir01, train_loss_step=3.100, train_acc_step=0.0909]



Epoch 1: 100%|██████████| 19/19 [00:04<00:00,  3.98it/s, v_num=ir01, train_loss_step=2.230, train_acc_step=0.455, valid_loss_step=2.420, valid_acc_step=0.400, valid_loss_epoch=2.420, valid_acc_epoch=0.400, valid_loss=2.420, valid_acc=0.400, train_loss_epoch=3.340, train_acc_epoch=0.0852]



Epoch 2: 100%|██████████| 19/19 [00:04<00:00,  4.03it/s, v_num=ir01, train_loss_step=2.350, train_acc_step=0.455, valid_loss_step=1.670, valid_acc_step=0.667, valid_loss_epoch=1.670, valid_acc_epoch=0.667, valid_loss=1.670, valid_acc=0.667, train_loss_epoch=2.400, train_acc_epoch=0.348]



Epoch 3: 100%|██████████| 19/19 [00:04<00:00,  4.02it/s, v_num=ir01, train_loss_step=1.330, train_acc_step=0.636, valid_loss_step=1.150, valid_acc_step=0.767, valid_loss_epoch=1.150, valid_acc_epoch=0.767, valid_loss=1.150, valid_acc=0.767, train_loss_epoch=1.780, train_acc_epoch=0.526]


Epoch 4: 100%|██████████| 19/19 [00:04<00:00,  3.86it/s, v_num=ir01, train_loss_step=1.020, train_acc_step=0.636, valid_loss_step=0.984, valid_acc_step=0.733, valid_loss_epoch=0.984, valid_acc_epoch=0.733, valid_loss=0.984, valid_acc=0.733, train_loss_epoch=1.550, train_acc_epoch=0.554]



Epoch 5: 100%|██████████| 19/19 [00:04<00:00,  3.90it/s, v_num=ir01, train_loss_step=1.210, train_acc_step=0.727, valid_loss_step=0.807, valid_acc_step=0.733, valid_loss_epoch=0.807, valid_acc_epoch=0.733, valid_loss=0.807, valid_acc=0.733, train_loss_epoch=1.420, train_acc_epoch=0.603]





Epoch 6: 100%|██████████| 19/19 [00:04<00:00,  3.86it/s, v_num=ir01, train_loss_step=0.871, train_acc_step=0.727, valid_loss_step=0.805, valid_acc_step=0.733, valid_loss_epoch=0.805, valid_acc_epoch=0.733, valid_loss=0.805, valid_acc=0.733, train_loss_epoch=1.240, train_acc_epoch=0.639]




Epoch 7: 100%|██████████| 19/19 [00:04<00:00,  3.81it/s, v_num=ir01, train_loss_step=0.850, train_acc_step=0.727, valid_loss_step=0.742, valid_acc_step=0.800, valid_loss_epoch=0.742, valid_acc_epoch=0.800, valid_loss=0.742, valid_acc=0.800, train_loss_epoch=1.160, train_acc_epoch=0.671]



Epoch 8: 100%|██████████| 19/19 [00:04<00:00,  3.81it/s, v_num=ir01, train_loss_step=0.629, train_acc_step=0.818, valid_loss_step=0.691, valid_acc_step=0.867, valid_loss_epoch=0.691, valid_acc_epoch=0.867, valid_loss=0.691, valid_acc=0.867, train_loss_epoch=1.050, train_acc_epoch=0.695]



Epoch 9: 100%|██████████| 19/19 [00:04<00:00,  3.80it/s, v_num=ir01, train_loss_step=1.010, train_acc_step=0.545, valid_loss_step=0.705, valid_acc_step=0.733, valid_loss_epoch=0.705, valid_acc_epoch=0.733, valid_loss=0.705, valid_acc=0.733, train_loss_epoch=0.964, train_acc_epoch=0.705]




Epoch 10: 100%|██████████| 19/19 [00:04<00:00,  3.82it/s, v_num=ir01, train_loss_step=0.868, train_acc_step=0.818, valid_loss_step=0.730, valid_acc_step=0.767, valid_loss_epoch=0.730, valid_acc_epoch=0.767, valid_loss=0.730, valid_acc=0.767, train_loss_epoch=1.080, train_acc_epoch=0.670]


Epoch 10: 100%|██████████| 19/19 [00:10<00:00,  1.82it/s, v_num=ir01, train_loss_step=0.868, train_acc_step=0.818, valid_loss_step=0.745, valid_acc_step=0.733, valid_loss_epoch=0.745, valid_acc_epoch=0.733, valid_loss=0.745, valid_acc=0.733, train_loss_epoch=0.995, train_acc_epoch=0.697]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
c:\Users\tian_\AppData\Local\anaconda3\envs\Pytorch_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
c:\Users\tian_\AppData\Local\anaconda3\envs\Pytorch_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:436: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.


