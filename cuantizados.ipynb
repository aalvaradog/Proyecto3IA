{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "from clasificadorB1 import *\n",
    "\n",
    "autoencoder_checkpoint_path70 = r'C:\\Users\\tian_\\Desktop\\lightning_logs\\unet_autoencoderB2-70.pth'\n",
    "autoencoder_checkpoint_path90 = r'C:\\Users\\tian_\\Desktop\\lightning_logs\\unet_autoencoderB2-90.pth'\n",
    "\n",
    "def load_model(path):\n",
    "    model = AutoencoderVGG16(data_dir, num_classes, autoencoder_checkpoint_path70)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "# Load models\n",
    "model1 = load_model('model1.pth')\n",
    "model2 = load_model('model2.pth')\n",
    "model3 = load_model('model3.pth')\n",
    "\n",
    "# Funcion para cuantizar el modelo\n",
    "def quantize_model(model):\n",
    "    model.eval()\n",
    "    model.fuse_model()\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "    return model\n",
    "\n",
    "# Fuse and quantize the models\n",
    "quantized_model1 = quantize_model(model1)\n",
    "quantized_model2 = quantize_model(model2)\n",
    "quantized_model3 = quantize_model(model3)\n",
    "\n",
    "# Save the quantized models\n",
    "torch.save(quantized_model1.state_dict(), 'quantized_model1.pth')\n",
    "torch.save(quantized_model2.state_dict(), 'quantized_model2.pth')\n",
    "torch.save(quantized_model3.state_dict(), 'quantized_model3.pth')\n",
    "\n",
    "\n",
    "### Paso 3: Comparar latencias, tamaños y rendimientos\n",
    "\n",
    "import time\n",
    "\n",
    "def measure_latency(model, dataloader):\n",
    "    latencies = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            start_time = time.time()\n",
    "            outputs = model(inputs)\n",
    "            end_time = time.time()\n",
    "            latencies.append(end_time - start_time)\n",
    "    return sum(latencies) / len(latencies)\n",
    "\n",
    "def measure_size(model_path):\n",
    "    return os.path.getsize(model_path)\n",
    "\n",
    "def measure_performance(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Medir latencias, tamaños y rendimientos de los modelos\n",
    "def evaluate_model(model, dataloader, path):\n",
    "    latency = measure_latency(model, dataloader)\n",
    "    size = measure_size(path)\n",
    "    accuracy = measure_performance(model, dataloader)\n",
    "    return latency, size, accuracy\n",
    "\n",
    "# Suponiendo que tienes los dataloaders 'dataloader_train' y 'dataloader_test'\n",
    "original_metrics = [\n",
    "    evaluate_model(model1, dataloader_test, 'model1.pth'),\n",
    "    evaluate_model(model2, dataloader_test, 'model2.pth'),\n",
    "    evaluate_model(model3, dataloader_test, 'model3.pth')\n",
    "]\n",
    "\n",
    "quantized_metrics = [\n",
    "    evaluate_model(quantized_model1, dataloader_test, 'quantized_model1.pth'),\n",
    "    evaluate_model(quantized_model2, dataloader_test, 'quantized_model2.pth'),\n",
    "    evaluate_model(quantized_model3, dataloader_test, 'quantized_model3.pth')\n",
    "]\n",
    "\n",
    "# Print results\n",
    "for i, (orig, quant) in enumerate(zip(original_metrics, quantized_metrics)):\n",
    "    print(f\"Model {i+1} Original - Latency: {orig[0]:.4f} s, Size: {orig[1]} bytes, Accuracy: {orig[2]:.2f}%\")\n",
    "    print(f\"Model {i+1} Quantized - Latency: {quant[0]:.4f} s, Size: {quant[1]} bytes, Accuracy: {quant[2]:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
